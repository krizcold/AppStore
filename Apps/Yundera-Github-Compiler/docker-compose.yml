name: yunderagithubcompiler

services:
  yunderagithubcompiler:
    image: krizcold/yundera-github-compiler:latest
    container_name: yunderagithubcompiler
    restart: unless-stopped
    expose:
      - "3000"
    entrypoint: ["/bin/sh"]
    command:
      - -c
      - |
        npm run setup
    environment:
      # --> GitHub token for repository access
      # REPO_0: "https://<TOKEN>@github.com/yourorg/private-repo-1.git"
      # REPO_1: "https://<TOKEN>@github.com/yourorg/private-repo-2.git"
      # --> Optional: disable auto-update on a per-repo basis
      # REPO_0_AUTOUPDATE: "true"

      # --> Optional: Run a custom command on run
      # DIAG_COMMAND: ""

      UPDATE_INTERVAL: "3600"
      FORCE_UPDATE_GLOBAL: "true"
      WEBUI_PORT: "3000"
      
      # AppStore deployment mode
      DEPLOYMENT_MODE: "appstore"
      CASAOS_API_HOST: "localhost"
      CASAOS_API_PORT: "8080"
      
      # Add CasaOS-like environment variables (from NSL setup)
      PUID: ${PUID}
      PGID: ${PGID}
      DATA_ROOT: "/DATA"
      REF_NET: "pcs"

    volumes:        
      # cloned repos
      - type: bind
        source: /DATA/AppData/yunderagithubcompiler/repos
        target: /app/repos

      # Mount DATA directory as read-only
      - type: bind
        source: /DATA
        target: /DATA
        read_only: true

      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock

    # Connect to the same network as the CasaOS service
    networks:
      - pcs
    
    # Add privileges to access CasaOS data (similar to CasaOS container)
    privileged: true
    
    # Add capabilities
    cap_add:
      - SYS_ADMIN
      - NET_ADMIN

    x-casaos:
      volumes:
        - container: /app/repos
          description:
            en_us: "Git repos are cloned here."
        - container: /DATA
          description:
            en_us: "Access to DATA directory (read-only via pre-install setup"

# Define the network as external, since it's created by the main NSL stack
networks:
  pcs:
    external: true

x-casaos:
  architectures:
    - amd64
    - arm64
  main: yunderagithubcompiler
  author: krizcold
  developer: krizcold
  icon: https://github.com/krizcold/Yundera-Github-Compiler/blob/main/YunderaCompiler.png?raw=true
  tagline:
    en_us: "Automatically build and deploy GitHub repos on Yundera"
  category: Utilities
  description:
    en_us: "Clone, build, and run Docker-based projects directly from GitHub URLs."
  title:
    en_us: "Yundera GitHub Compiler"
  store_app_id: yunderagithubcompiler
  is_uncontrolled: false
  index: /
  webui_port: 3000
  pre-install-cmd: |
    # Create a simplified watcher that waits for compose file and fixes docker.sock
    cat > /tmp/yundera-docker-sock-fixer.sh << 'EOF'
    #!/bin/bash
    echo "ðŸ”„ Yundera docker.sock fixer started at $(date)"
    
    COMPOSE_FILE="/DATA/AppData/casaos/apps/yunderagithubcompiler/docker-compose.yml"
    CONTAINER_NAME="yunderagithubcompiler"
    
    echo "ðŸ“ Target compose file: $COMPOSE_FILE"
    echo "ðŸ³ Target container: $CONTAINER_NAME"
    
    # Step 1: Wait for compose file to exist
    echo "ðŸ” Step 1: Waiting for compose file to exist..."
    counter=0
    max_wait=60  # 60 seconds max wait
    
    while [ $counter -lt $max_wait ]; do
      if [ -f "$COMPOSE_FILE" ]; then
        echo "âœ… Compose file exists after $counter seconds"
        break
      fi
      echo "â³ Compose file not found yet... (${counter}s/${max_wait}s)"
      sleep 1
      counter=$((counter + 1))
    done
    
    if [ ! -f "$COMPOSE_FILE" ]; then
      echo "âŒ Compose file not found after ${max_wait}s, exiting"
      exit 1
    fi
    
    # Step 2: Check if docker.sock is already mounted (EXCLUDING pre-install-cmd section)
    echo "ðŸ” Step 2: Checking docker.sock mount status in volumes section only..."
    
    # Extract only the service section (before x-casaos section)
    SERVICE_SECTION=$(awk '/^[[:space:]]*yunderagithubcompiler:/,/^[[:space:]]*x-casaos:/ {
      if (/^[[:space:]]*x-casaos:/) exit;
      print
    }' "$COMPOSE_FILE")
    
    echo "ðŸ“‹ Service section extracted for analysis:"
    echo "$SERVICE_SECTION"
    
    # Check if docker.sock is in the service volumes section
    if echo "$SERVICE_SECTION" | grep -q "/var/run/docker.sock:/var/run/docker.sock"; then
      echo "âœ… Docker.sock is already mounted in service volumes section"
    else
      echo "âŒ Docker.sock is NOT mounted in service volumes section"
      
      # Step 3: Add docker.sock mount
      echo "ðŸ”§ Step 3: Adding docker.sock mount to compose file..."
      
      # Backup original
      cp "$COMPOSE_FILE" "$COMPOSE_FILE.backup"
      echo "ðŸ“‹ Backed up compose file to $COMPOSE_FILE.backup"
      
      # Check if volumes section exists in service
      if echo "$SERVICE_SECTION" | grep -q "volumes:"; then
        echo "ðŸ“ Found existing volumes section in service"
        
        # DEBUG: Check permissions and file status
        echo "ðŸ” DEBUG: Checking file permissions and access..."
        ls -la "$COMPOSE_FILE"
        echo "ðŸ” DEBUG: Current user and groups:"
        id
        echo "ðŸ” DEBUG: File ownership:"
        stat "$COMPOSE_FILE"
        echo "ðŸ” DEBUG: Can we write to the file?"
        if [ -w "$COMPOSE_FILE" ]; then
          echo "âœ… File is writable"
        else
          echo "âŒ File is NOT writable"
        fi
        
        # DEBUG: Test if sed pattern matches
        echo "ðŸ” DEBUG: Testing if sed pattern matches..."
        if grep -n "read_only:[[:space:]]*true" "$COMPOSE_FILE"; then
          echo "âœ… Pattern 'read_only: true' found in file"
        else
          echo "âŒ Pattern 'read_only: true' NOT found in file"
          echo "ðŸ” DEBUG: Looking for any 'read_only' patterns..."
          grep -n "read_only" "$COMPOSE_FILE" || echo "No read_only found at all"
        fi
        
        # DEBUG: Try a simpler sed command first
        echo "ðŸ” DEBUG: Trying simple sed test..."
        sed -i 's/read_only: true/read_only: true/' "$COMPOSE_FILE"
        if [ $? -eq 0 ]; then
          echo "âœ… Simple sed command worked"
        else
          echo "âŒ Simple sed command failed with exit code: $?"
        fi
        
        # Now try the actual sed command with debugging
        echo "ðŸ” DEBUG: Executing main sed command..."
        sed -i '/^[[:space:]]*volumes:/,/^[[:space:]]*networks:/ {
          /^[[:space:]]*read_only:[[:space:]]*true/ {
            a\            - type: bind\n              source: /var/run/docker.sock\n              target: /var/run/docker.sock
          }
        }' "$COMPOSE_FILE"
        
        SED_EXIT_CODE=$?
        echo "ðŸ” DEBUG: Sed command exit code: $SED_EXIT_CODE"
        
        if [ $SED_EXIT_CODE -eq 0 ]; then
          echo "âœ… Sed command executed successfully"
        else
          echo "âŒ Sed command failed with exit code: $SED_EXIT_CODE"
        fi
        
        echo "âœ… Added docker.sock mount to existing volumes section"
      else
        echo "ðŸ“ No volumes section found in service, adding one"
        
        # Add volumes section with docker.sock mount before networks
        sed -i '/^[[:space:]]*yunderagithubcompiler:/,/^[[:space:]]*networks:/ {
          /^[[:space:]]*networks:/ i\    volumes:\n        - type: bind\n          source: /var/run/docker.sock\n          target: /var/run/docker.sock\n
        }' "$COMPOSE_FILE"
        echo "âœ… Added volumes section with docker.sock mount"
      fi
      
      # Verify the change by checking service section again
      UPDATED_SERVICE_SECTION=$(awk '/^[[:space:]]*yunderagithubcompiler:/,/^[[:space:]]*x-casaos:/ {
        if (/^[[:space:]]*x-casaos:/) exit;
        print
      }' "$COMPOSE_FILE")
      
      echo "ðŸ“‹ Updated service section:"
      echo "$UPDATED_SERVICE_SECTION"
      
      # Check for docker.sock mount in the updated service section
      if echo "$UPDATED_SERVICE_SECTION" | grep -q "source: /var/run/docker.sock"; then
        echo "âœ… Docker.sock mount successfully added to service volumes section"
      else
        echo "âŒ Failed to add docker.sock mount to service volumes section"
        exit 1
      fi
    fi
    
    # Step 4: Wait for container to be created
    echo "ðŸ” Step 4: Waiting for container to be created..."
    counter=0
    max_wait=30  # 30 seconds max wait
    
    while [ $counter -lt $max_wait ]; do
      if docker ps -a --filter "name=$CONTAINER_NAME" --format "{{.Names}}" | grep -q "^$CONTAINER_NAME$"; then
        echo "âœ… Container $CONTAINER_NAME found after $counter seconds"
        break
      fi
      echo "â³ Container not found yet... (${counter}s/${max_wait}s)"
      sleep 1
      counter=$((counter + 1))
    done
    
    if ! docker ps -a --filter "name=$CONTAINER_NAME" --format "{{.Names}}" | grep -q "^$CONTAINER_NAME$"; then
      echo "âŒ Container $CONTAINER_NAME not found after ${max_wait}s"
      echo "ðŸ“‹ Available containers:"
      docker ps -a --format "table {{.Names}}\t{{.Status}}\t{{.Image}}"
      exit 1
    fi
    
    # Step 5: Restart the container with new compose file
    echo "ðŸ”„ Step 5: Restarting container with updated compose file..."
    
    # Get container status
    CONTAINER_STATUS=$(docker ps -a --filter "name=$CONTAINER_NAME" --format "{{.Status}}")
    echo "ðŸ“Š Container status: $CONTAINER_STATUS"
    
    # Try Method 1: Gentle restart (just restart the container)
    echo "ðŸ”„ Method 1: Attempting gentle restart..."
    if docker restart "$CONTAINER_NAME" 2>/dev/null; then
      echo "âœ… Container restarted successfully with gentle method"
      sleep 3
      
      # Check if docker.sock is accessible after restart
      if docker exec "$CONTAINER_NAME" test -S /var/run/docker.sock 2>/dev/null; then
        echo "âœ… Docker.sock is accessible after gentle restart - SUCCESS!"
      else
        echo "âŒ Docker.sock still not accessible after gentle restart"
        echo "ðŸ”„ Falling back to Method 2..."
        
        # Method 2: Stop and recreate with docker compose v2
        echo "ðŸ›‘ Stopping container $CONTAINER_NAME..."
        docker stop "$CONTAINER_NAME" || echo "âš ï¸  Container might not be running"
        
        echo "ðŸ—‘ï¸  Removing container $CONTAINER_NAME..."
        docker rm "$CONTAINER_NAME" || echo "âš ï¸  Container might not exist"
        
        echo "ðŸš€ Recreating container with docker compose v2..."
        cd "/DATA/AppData/casaos/apps/yunderagithubcompiler"
        docker compose up -d
      fi
    else
      echo "âŒ Gentle restart failed, trying Method 2..."
      
      # Method 2: Stop and recreate with docker compose v2
      echo "ðŸ›‘ Stopping container $CONTAINER_NAME..."
      docker stop "$CONTAINER_NAME" || echo "âš ï¸  Container might not be running"
      
      echo "ðŸ—‘ï¸  Removing container $CONTAINER_NAME..."
      docker rm "$CONTAINER_NAME" || echo "âš ï¸  Container might not exist"
      
      echo "ðŸš€ Recreating container with docker compose v2..."
      cd "/DATA/AppData/casaos/apps/yunderagithubcompiler"
      docker compose up -d
    fi
    
    # Verify the restart
    echo "âœ… Step 6: Verifying restart..."
    sleep 5
    
    NEW_STATUS=$(docker ps --filter "name=$CONTAINER_NAME" --format "{{.Status}}")
    if [ -n "$NEW_STATUS" ]; then
      echo "âœ… Container $CONTAINER_NAME is now running: $NEW_STATUS"
      
      # Check if docker.sock is actually mounted
      if docker exec "$CONTAINER_NAME" test -S /var/run/docker.sock 2>/dev/null; then
        echo "âœ… Docker.sock is successfully mounted and accessible in container"
      else
        echo "âŒ Docker.sock is NOT accessible in container"
      fi
    else
      echo "âŒ Container $CONTAINER_NAME is not running after restart"
    fi
    
    echo "ðŸ Watcher script completed at $(date)"
    EOF
    
    # Make it executable and run in background
    chmod +x /tmp/yundera-docker-sock-fixer.sh
    nohup /tmp/yundera-docker-sock-fixer.sh > /tmp/yundera-docker-sock-fixer.log 2>&1 &
    
    echo "ðŸš€ Docker.sock fixer launched, check logs at /tmp/yundera-docker-sock-fixer.log"
