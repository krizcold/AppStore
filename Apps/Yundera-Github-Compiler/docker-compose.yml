name: yunderagithubcompiler

services:
  yunderagithubcompiler:
    image: krizcold/yundera-github-compiler:latest
    container_name: yunderagithubcompiler
    restart: unless-stopped
    expose:
      - "3000"
    user: "root"
    entrypoint: ["/bin/sh"]
    command:
      - -c
      - |
        npm run setup
    environment:
      # Application settings
      WEBUI_PORT: "3000"
      
      # CasaOS integration
      DEPLOYMENT_MODE: "appstore"
      CASAOS_API_HOST: "localhost"
      CASAOS_API_PORT: "8080"
      DATA_ROOT: $DATA_ROOT
      
      # Yundera platform integration
      DOMAIN: $DOMAIN
      PROVIDER_STR: $PROVIDER_STR
      UID: $UID
      DEFAULT_PWD: $DEFAULT_PWD
      PUBLIC_IP: $PUBLIC_IP
      DEFAULT_USER: $DEFAULT_USER
      
      # Authentication and paths
      JWT_SECRET: $JWT_SECRET
      AUTHORITY_ENDPOINT: $AUTHORITY_ENDPOINT
      COMPOSE_FOLDER_PATH: $COMPOSE_FOLDER_PATH
      BASE_PATH: $BASE_PATH
      MOCK: $MOCK
      
      # Pass through the host's CasaOS environment variables.
      # These will be populated by the AppStore during installation.
      PUID: $PUID
      PGID: $PGID
      REF_DOMAIN: $REF_DOMAIN
      REF_NET: $REF_NET
      REF_PORT: $REF_PORT
      REF_SCHEME: $REF_SCHEME
      REF_SEPARATOR: $REF_SEPARATOR
      
      # Debug/logging settings
      LOG_APPS_BEACON: $LOG_APPS_BEACON
      
      user: $user
      default_pwd: $default_pwd
      public_ip: $public_ip

    volumes:        
      # cloned repos
      - type: bind
        source: /DATA/AppData/yunderagithubcompiler/repos
        target: /app/repos

      # persistent UI data storage
      - type: bind
        source: /DATA/AppData/yunderagithubcompiler/uidata
        target: /app/uidata

      # Writable mount for app metadata
      - type: bind
        source: /DATA/AppData
        target: /DATA/AppData

      # Bind mount for the main app directory
      - type: bind
        source: /DATA/AppData/casaos/apps/yunderagithubcompiler
        target: /DATA/AppData/casaos/apps/yunderagithubcompiler
        read_only: true

      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock

    # Connect to the same network as the CasaOS service
    networks:
      - pcs
    
    # Add privileges to access CasaOS data (similar to CasaOS container)
    privileged: true
    
    # Add capabilities
    cap_add:
      - SYS_ADMIN
      - NET_ADMIN

    x-casaos:
      volumes:
        - container: /app/repos
          description:
            en_us: "Git repos are cloned here."
        - container: /app/uidata
          description:
            en_us: "Persistent UI data storage."

# Define the network as external, since it's created by the main NSL stack
networks:
  pcs:
    external: true

x-casaos:
  architectures:
    - amd64
    - arm64
  main: yunderagithubcompiler
  author: krizcold
  developer: krizcold
  icon: https://github.com/krizcold/Yundera-Github-Compiler/blob/main/YunderaCompiler.png?raw=true
  tagline:
    en_us: "Automatically build and deploy GitHub repos on Yundera"
  category: Utilities
  description:
    en_us: "Clone, build, and run Docker-based projects directly from GitHub URLs."
  title:
    en_us: "Yundera GitHub Compiler"
  store_app_id: yunderagithubcompiler
  is_uncontrolled: false
  index: /
  webui_port: 3000
  pre-install-cmd: |
    # Fail on command errors, but allow missing envs
    set -e

    # Generate the hash value that will be embedded into the background script.
    if command -v openssl >/dev/null 2>&1; then
      AUTH_HASH=$(openssl rand -hex 64)
    else
      # Fallback if openssl is unavailable
      AUTH_HASH=$(dd if=/dev/urandom bs=32 count=1 2>/dev/null | od -An -tx1 | tr -d ' \n')
    fi

    echo "ðŸš€ Yundera GitHub Compiler pre-install starting..."

    # -------------------------
    # SSH setup (strict where it matters, tolerant where it can vary by host)
    # -------------------------
    SSH_SETUP_LOG="/tmp/yundera-ssh-setup-$(date +%Y%m%d-%H%M%S).log"
    echo "ðŸ”§ Setting up SSH access (logging to $SSH_SETUP_LOG)..."

    {
      echo "=== SSH Setup Started at $(date) ==="
      echo "Host: $(hostname)"
      echo "User: $(whoami)"
      echo "UID/GID: $(id)"
      echo "Working directory: $(pwd)"
      echo

      BASE_DIR="/DATA/AppData/yunderagithubcompiler"
      UIDATA_DIR="$BASE_DIR/uidata"
      SSH_DIR="$UIDATA_DIR/.ssh"

      echo "ðŸ“ Ensuring directories exist..."
      mkdir -p "$SSH_DIR"

      # Prefer CasaOS-provided IDs if present; otherwise best effort (do NOT fail if chown is not possible)
      OWNER_UID="${PUID:-}"
      OWNER_GID="${PGID:-}"
      if [ -z "${OWNER_UID}" ] || [ -z "${OWNER_GID}" ]; then
        # Try 'ubuntu' if it exists; else skip
        if id ubuntu >/dev/null 2>&1; then
          OWNER_UID="$(id -u ubuntu)"
          # 988 is docker group on many CasaOS setups, but chown will still succeed if group doesn't exist; guard anyway
          OWNER_GID="${PGID:-$(id -g ubuntu)}"
        fi
      fi

      if [ -n "${OWNER_UID:-}" ] && [ -n "${OWNER_GID:-}" ]; then
        chown -R "${OWNER_UID}:${OWNER_GID}" "$BASE_DIR" 2>/dev/null || true
      fi

      # Permissions (donâ€™t fail hard on chmod differences)
      chmod 755 "$BASE_DIR" 2>/dev/null || true
      chmod 755 "$UIDATA_DIR" 2>/dev/null || true
      chmod 700 "$SSH_DIR" 2>/dev/null || true

      # Hard requirement: we must be able to write in $SSH_DIR
      if ! ( touch "$SSH_DIR/.perm_test" && rm -f "$SSH_DIR/.perm_test" ); then
        echo "âŒ $SSH_DIR is not writable; aborting pre-install."
        exit 1
      fi
      echo "âœ… $SSH_DIR is writable."

      echo
      echo "ðŸ” Checking/generating SSH key pair..."
      if [ ! -f "$SSH_DIR/id_ed25519" ]; then
        if command -v ssh-keygen >/dev/null 2>&1; then
          ssh-keygen -t ed25519 -f "$SSH_DIR/id_ed25519" -N "" -C "yundera-compiler@container" || {
            echo "âŒ ssh-keygen failed unexpectedly; aborting."
            exit 1
          }
          chmod 600 "$SSH_DIR/id_ed25519" 2>/dev/null || true
          chmod 644 "$SSH_DIR/id_ed25519.pub" 2>/dev/null || true
          echo "âœ… SSH key generated."
        else
          echo "âš ï¸ ssh-keygen not found; skipping key generation (non-fatal)."
        fi
      else
        echo "âœ… SSH key already exists."
      fi

      echo
      echo "ðŸ”‘ authorized_keys (best effort, non-fatal)..."
      PUBKEY=""
      [ -f "$SSH_DIR/id_ed25519.pub" ] && PUBKEY="$(cat "$SSH_DIR/id_ed25519.pub" 2>/dev/null || true)"
      if [ -n "$PUBKEY" ]; then
        # Choose a typical non-root user with a /home dir if available
        TARGET_SSH_USER="${TARGET_SSH_USER:-}"
        if [ -z "$TARGET_SSH_USER" ]; then
          TARGET_SSH_USER="$(awk -F: '$3>=1000 && $3<65534 && $6 ~ /^\/home\// {print $1; exit}' /etc/passwd 2>/dev/null || true)"
          [ -z "$TARGET_SSH_USER" ] && [ -d /home/ubuntu ] && TARGET_SSH_USER="ubuntu" || true
        fi
        if [ -n "$TARGET_SSH_USER" ] && getent passwd "$TARGET_SSH_USER" >/dev/null 2>&1; then
          TARGET_HOME="$(getent passwd "$TARGET_SSH_USER" | cut -d: -f6)"
          mkdir -p "$TARGET_HOME/.ssh" 2>/dev/null || true
          chmod 700 "$TARGET_HOME/.ssh" 2>/dev/null || true
          AUTH_KEYS="$TARGET_HOME/.ssh/authorized_keys"
          touch "$AUTH_KEYS" 2>/dev/null || true
          chmod 600 "$AUTH_KEYS" 2>/dev/null || true
          if ! grep -Fq "$PUBKEY" "$AUTH_KEYS" 2>/dev/null; then
            echo "$PUBKEY" >> "$AUTH_KEYS" 2>/dev/null || true
            echo "âœ… Public key added to $TARGET_SSH_USER authorized_keys"
          else
            echo "âœ… Public key already present"
          fi
          # Ownership back to target user (non-fatal)
          if id "$TARGET_SSH_USER" >/dev/null 2>&1; then
            chown -R "$(id -u "$TARGET_SSH_USER")":"$(id -g "$TARGET_SSH_USER")" "$TARGET_HOME/.ssh" 2>/dev/null || true
          fi
        else
          echo "â„¹ï¸ No suitable non-root user found; skipping authorized_keys setup."
        fi
      else
        echo "â„¹ï¸ No public key available; skipping authorized_keys setup."
      fi

      echo
      echo "=== SSH Setup Completed at $(date) ==="
    } > "$SSH_SETUP_LOG" 2>&1

    echo "âœ… SSH setup completed (see $SSH_SETUP_LOG)."

    # -------------------------------------------------
    # Watcher: unchanged logic, just written and launched
    # -------------------------------------------------
    cat > /tmp/yundera-docker-sock-fixer.sh << EOF
    #!/bin/bash
    echo "ðŸ”„ Yundera docker.sock fixer started at \$(date)"
    
    COMPOSE_FILE="/DATA/AppData/casaos/apps/yunderagithubcompiler/docker-compose.yml"
    CONTAINER_NAME="yunderagithubcompiler"
    
    echo "ðŸ“ Target compose file: \$COMPOSE_FILE"
    echo "ðŸ³ Target container: \$CONTAINER_NAME"
    
    # Get Docker group ID from host system
    echo "ðŸ” Detecting Docker group ID..."
    if [ -S /var/run/docker.sock ]; then
      DOCKER_GID=\$(stat -c '%g' /var/run/docker.sock)
      echo "âœ… Docker group ID detected: \$DOCKER_GID"
    else
      echo "âŒ Docker socket not found on host, using fallback GID"
      DOCKER_GID=999
    fi
    
    # Step 1: Wait for compose file to exist
    echo "ðŸ” Step 1: Waiting for compose file to exist..."
    counter=0
    max_wait=120  # 120 seconds max wait (2 minutes)
    
    while [ \$counter -lt \$max_wait ]; do
      if [ -f "\$COMPOSE_FILE" ]; then
        echo "âœ… Compose file exists after \$counter seconds"
        break
      fi
      echo "â³ Compose file not found yet... (\${counter}s/\${max_wait}s)"
      sleep 2
      counter=\$((counter + 2))
    done
    
    if [ ! -f "\$COMPOSE_FILE" ]; then
      echo "âŒ Compose file not found after \${max_wait}s, exiting"
      exit 1
    fi
    
    # Step 2: Check if docker.sock is already mounted
    SERVICE_SECTION=\$(awk '/^[[:space:]]*yunderagithubcompiler:/,/^[[:space:]]*x-casaos:/ {
      if (/^[[:space:]]*x-casaos:/) exit;
      print
    }' "\$COMPOSE_FILE")
    
    if echo "\$SERVICE_SECTION" | grep -q "/var/run/docker.sock:/var/run/docker.sock"; then
      echo "âœ… Docker.sock is already mounted in service volumes section"
    else
      echo "âŒ Docker.sock is NOT mounted in service volumes section"
      
      # Step 3: Add docker.sock mount and environment variables
      echo "ðŸ”§ Step 3: Adding docker.sock mount to compose file..."
      
      cp "\$COMPOSE_FILE" "\$COMPOSE_FILE.backup"
      echo "ðŸ“‹ Backed up compose file to \$COMPOSE_FILE.backup"
      
      if echo "\$SERVICE_SECTION" | grep -q "volumes:"; then
        echo "ðŸ“ Found existing volumes section in service"
        
        sed -i '/^[[:space:]]*volumes:/,/^[[:space:]]*networks:/ {
          /^[[:space:]]*read_only:[[:space:]]*true/ {
            a\            - type: bind\n              source: /var/run/docker.sock\n              target: /var/run/docker.sock
          }
        }' "\$COMPOSE_FILE"
        
        echo "ðŸ”§ Adding Docker group ID and AUTH_HASH to environment variables..."
        
        sed -i "s/WEBUI_PORT: \".*\"/&\\n            DOCKER_GID: \"\$DOCKER_GID\"\\n            AUTH_HASH: \"${AUTH_HASH}\"/" "\$COMPOSE_FILE"
        
        echo "âœ… Added docker.sock mount and environment variables to existing sections."
      else
        echo "ðŸ“ No volumes section found in service, adding one"
        
        sed -i '/^[[:space:]]*yunderagithubcompiler:/,/^[[:space:]]*networks:/ {
          /^[[:space:]]*networks:/ i\    volumes:\n        - type: bind\n          source: /var/run/docker.sock\n          target: /var/run/docker.sock\n
        }' "\$COMPOSE_FILE"
        echo "âœ… Added volumes section with docker.sock mount"
      fi

      # Modify the index line to include the hash
      sed -i "s|index: /|index: /?hash=${AUTH_HASH}|" "\$COMPOSE_FILE"

      # Cleanly remove the entire pre-install-cmd line from the final compose file
      echo "ðŸ”§ Removing pre-install-cmd from final compose file..."
      sed -i '/^[[:space:]]*pre-install-cmd:/d' "\$COMPOSE_FILE"
    fi
    
    # Step 4: Wait for container to be created
    echo "ðŸ” Step 4: Waiting for container to be created..."
    counter=0
    max_wait=60  # 60 seconds max wait
    
    while [ \$counter -lt \$max_wait ]; do
      if docker ps -a --filter "name=\$CONTAINER_NAME" --format "{{.Names}}" | grep -q "^\$CONTAINER_NAME\$"; then
        echo "âœ… Container \$CONTAINER_NAME found after \$counter seconds"
        break
      fi
      echo "â³ Container not found yet... (\${counter}s/\${max_wait}s)"
      sleep 2
      counter=\$((counter + 2))
    done
    
    if ! docker ps -a --filter "name=\$CONTAINER_NAME" --format "{{.Names}}" | grep -q "^\$CONTAINER_NAME\$"; then
      echo "âŒ Container \$CONTAINER_NAME not found after \${max_wait}s"
      echo "ðŸ“‹ Available containers:"
      docker ps -a --format "table {{.Names}}\t{{.Status}}\t{{.Image}}"
      exit 1
    fi
    
    # Step 5: Restart the container with new compose file
    echo "ðŸ”„ Step 5: Restarting container with updated compose file..."
    
    cd "/DATA/AppData/casaos/apps/yunderagithubcompiler"
    if docker compose version >/dev/null 2>&1; then
      docker compose up -d --force-recreate
    elif command -v docker-compose >/dev/null 2>&1; then
      docker-compose up -d --force-recreate
    else
      echo "âš ï¸ No docker compose binary found; skipping restart"
      exit 0
    fi
    
    # Step 6: Verify the restart
    echo "âœ… Step 6: Verifying restart..."
    sleep 5
    
    NEW_STATUS=\$(docker ps --filter "name=\$CONTAINER_NAME" --format "{{.Status}}")
    if [ -n "\$NEW_STATUS" ]; then
      echo "âœ… Container \$CONTAINER_NAME is now running: \$NEW_STATUS"
      
      if docker exec "\$CONTAINER_NAME" test -S /var/run/docker.sock 2>/dev/null; then
        echo "âœ… Docker.sock is successfully mounted and accessible in container"
      else
        echo "âŒ Docker.sock is NOT accessible in container"
      fi
    else
      echo "âŒ Container \$CONTAINER_NAME is not running after restart"
    fi
    
    echo "ðŸ Watcher script completed at \$(date)"
    EOF

    chmod +x /tmp/yundera-docker-sock-fixer.sh
    nohup /tmp/yundera-docker-sock-fixer.sh > /tmp/yundera-docker-sock-fixer.log 2>&1 &

    echo "ðŸš€ Docker.sock fixer launched, check logs at /tmp/yundera-docker-sock-fixer.log"
