name: yunderagithubcompiler

services:
  yunderagithubcompiler:
    image: krizcold/yundera-github-compiler:latest
    container_name: yunderagithubcompiler
    restart: unless-stopped
    expose:
      - "3000"
    user: "root"
    entrypoint: ["/bin/sh"]
    command:
      - -c
      - |
        npm run setup
    environment:
      # Application settings
      WEBUI_PORT: "3000"
      
      # CasaOS integration
      DEPLOYMENT_MODE: "appstore"
      CASAOS_API_HOST: "localhost"
      CASAOS_API_PORT: "8080"
      DATA_ROOT: $DATA_ROOT
      
      # Yundera platform integration
      DOMAIN: $DOMAIN
      PROVIDER_STR: $PROVIDER_STR
      UID: $UID
      DEFAULT_PWD: $DEFAULT_PWD
      PUBLIC_IP: $PUBLIC_IP
      DEFAULT_USER: $DEFAULT_USER
      
      # Authentication and paths
      JWT_SECRET: $JWT_SECRET
      AUTHORITY_ENDPOINT: $AUTHORITY_ENDPOINT
      COMPOSE_FOLDER_PATH: $COMPOSE_FOLDER_PATH
      BASE_PATH: $BASE_PATH
      MOCK: $MOCK
      
      # Pass through the host's CasaOS environment variables.
      # These will be populated by the AppStore during installation.
      PUID: $PUID
      PGID: $PGID
      REF_DOMAIN: $REF_DOMAIN
      REF_NET: $REF_NET
      REF_PORT: $REF_PORT
      REF_SCHEME: $REF_SCHEME
      REF_SEPARATOR: $REF_SEPARATOR
      
      # Debug/logging settings
      LOG_APPS_BEACON: $LOG_APPS_BEACON
      
      user: $user
      default_pwd: $default_pwd
      public_ip: $public_ip

    volumes:        
      # cloned repos
      - type: bind
        source: /DATA/AppData/yunderagithubcompiler/repos
        target: /app/repos

      # persistent UI data storage
      - type: bind
        source: /DATA/AppData/yunderagithubcompiler/uidata
        target: /app/uidata

      # Writable mount for app metadata
      - type: bind
        source: /DATA/AppData
        target: /DATA/AppData

      # Bind mount for the main app directory
      - type: bind
        source: /DATA/AppData/casaos/apps/yunderagithubcompiler
        target: /DATA/AppData/casaos/apps/yunderagithubcompiler
        read_only: true

      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock

    # Connect to the same network as the CasaOS service
    networks:
      - pcs
    
    # Add privileges to access CasaOS data (similar to CasaOS container)
    privileged: true
    
    # Add capabilities
    cap_add:
      - SYS_ADMIN
      - NET_ADMIN

    x-casaos:
      volumes:
        - container: /app/repos
          description:
            en_us: "Git repos are cloned here."
        - container: /app/uidata
          description:
            en_us: "Persistent UI data storage."

# Define the network as external, since it's created by the main NSL stack
networks:
  pcs:
    external: true

x-casaos:
  architectures:
    - amd64
    - arm64
  main: yunderagithubcompiler
  author: krizcold
  developer: krizcold
  icon: https://github.com/krizcold/Yundera-Github-Compiler/blob/main/YunderaCompiler.png?raw=true
  tagline:
    en_us: "Automatically build and deploy GitHub repos on Yundera"
  category: Utilities
  description:
    en_us: "Clone, build, and run Docker-based projects directly from GitHub URLs."
  title:
    en_us: "Yundera GitHub Compiler"
  store_app_id: yunderagithubcompiler
  is_uncontrolled: false
  index: /
  webui_port: 3000
  pre-install-cmd: |
    # Fail on command errors, but allow optional envs to be unset
    set -e

    # Generate the hash value that will be embedded into the watcher and index
    if command -v openssl >/dev/null 2>&1; then
      AUTH_HASH="$(openssl rand -hex 64)"
    else
      AUTH_HASH="$(dd if=/dev/urandom bs=32 count=1 2>/dev/null | od -An -tx1 | tr -d ' \n')"
    fi

    echo "🚀 Yundera GitHub Compiler pre-install starting..."

    ########################################################################
    # SSH setup (best-effort: won’t mask real failures elsewhere)
    ########################################################################
    SSH_SETUP_LOG="/tmp/yundera-ssh-setup-$(date +%Y%m%d-%H%M%S).log"
    echo "🔧 Setting up SSH access (logging to $SSH_SETUP_LOG)..."

    {
      BASE_DIR="/DATA/AppData/yunderagithubcompiler"
      UIDATA_DIR="$BASE_DIR/uidata"
      SSH_DIR="$UIDATA_DIR/.ssh"

      # Use CasaOS-provided user/group if available
      OWNER_UID="${PUID:-}"
      OWNER_GID="${PGID:-}"

      mkdir -p "$SSH_DIR" || true
      [ -n "$OWNER_UID" ] && [ -n "$OWNER_GID" ] && chown -R "$OWNER_UID:$OWNER_GID" "$BASE_DIR" 2>/dev/null || true
      chmod 755 "$BASE_DIR" 2>/dev/null || true
      chmod 755 "$UIDATA_DIR" 2>/dev/null || true
      chmod 700 "$SSH_DIR" 2>/dev/null || true

      # Verify we can write in SSH dir (hard requirement for key creation)
      if ! ( touch "$SSH_DIR/.perm_test" && rm -f "$SSH_DIR/.perm_test" ); then
        echo "❌ $SSH_DIR is not writable; skipping SSH keygen."
      else
        echo "✅ $SSH_DIR writable."
        # Create key if missing (non-fatal if ssh-keygen missing)
        if [ ! -f "$SSH_DIR/id_ed25519" ]; then
          if command -v ssh-keygen >/dev/null 2>&1; then
            ssh-keygen -t ed25519 -f "$SSH_DIR/id_ed25519" -N "" -C "yundera-compiler@container" || true
            chmod 600 "$SSH_DIR/id_ed25519" 2>/dev/null || true
            chmod 644 "$SSH_DIR/id_ed25519.pub" 2>/dev/null || true
            echo "✅ SSH key generated."
          else
            echo "⚠️ ssh-keygen not found; skipping key generation."
          fi
        else
          echo "✅ SSH key already exists."
        fi
      fi

      # Authorized_keys on host (best-effort)
      if [ -f "$SSH_DIR/id_ed25519.pub" ]; then
        PUBKEY="$(cat "$SSH_DIR/id_ed25519.pub" 2>/dev/null || true)"
        TARGET_SSH_USER="${TARGET_SSH_USER:-}"
        if [ -z "$TARGET_SSH_USER" ]; then
          TARGET_SSH_USER="$(awk -F: '$3>=1000 && $3<65534 && $6 ~ /^\/home\// {print $1; exit}' /etc/passwd 2>/dev/null || true)"
          [ -z "$TARGET_SSH_USER" ] && [ -d /home/ubuntu ] && TARGET_SSH_USER="ubuntu" || true
        fi
        if [ -n "$TARGET_SSH_USER" ] && getent passwd "$TARGET_SSH_USER" >/dev/null 2>&1; then
          TARGET_HOME="$(getent passwd "$TARGET_SSH_USER" | cut -d: -f6)"
          mkdir -p "$TARGET_HOME/.ssh" 2>/dev/null || true
          chmod 700 "$TARGET_HOME/.ssh" 2>/dev/null || true
          AUTH_KEYS="$TARGET_HOME/.ssh/authorized_keys"
          touch "$AUTH_KEYS" 2>/dev/null || true
          chmod 600 "$AUTH_KEYS" 2>/dev/null || true
          if [ -n "$PUBKEY" ] && ! grep -Fq "$PUBKEY" "$AUTH_KEYS" 2>/dev/null; then
            echo "$PUBKEY" >> "$AUTH_KEYS" 2>/dev/null || true
            echo "✅ Public key added to $TARGET_SSH_USER authorized_keys"
          else
            echo "ℹ️ Public key already present or empty."
          fi
          if id "$TARGET_SSH_USER" >/dev/null 2>&1; then
            chown -R "$(id -u "$TARGET_SSH_USER")":"$(id -g "$TARGET_SSH_USER")" "$TARGET_HOME/.ssh" 2>/dev/null || true
          fi
        else
          echo "ℹ️ No suitable non-root user found; skipping authorized_keys."
        fi
      fi
    } > "$SSH_SETUP_LOG" 2>&1
    echo "✅ SSH setup completed (see $SSH_SETUP_LOG)."

    ########################################################################
    # Watcher: waits for CasaOS to settle, reuses the SAME compose project,
    # adds docker.sock + envs, removes pre-install-cmd line, then recreates.
    ########################################################################
    cat > /tmp/yundera-docker-sock-fixer.sh << 'EOF'
    #!/bin/sh
    set -e

    echo "🔄 Yundera docker.sock fixer started at $(date)"

    COMPOSE_FILE="/DATA/AppData/casaos/apps/yunderagithubcompiler/docker-compose.yml"
    APP_DIR="/DATA/AppData/casaos/apps/yunderagithubcompiler"
    SERVICE_NAME="yunderagithubcompiler"
    LOCKDIR="/tmp/yunderagithubcompiler.lock"

    # single-instance lock
    if ! mkdir "$LOCKDIR" 2>/dev/null; then
      echo "ℹ️ Watcher already running; exiting."
      exit 0
    fi
    trap 'rmdir "$LOCKDIR"' EXIT

    # Detect Docker GID
    if [ -S /var/run/docker.sock ]; then
      if stat -c '%g' /var/run/docker.sock >/dev/null 2>&1; then
        DOCKER_GID="$(stat -c '%g' /var/run/docker.sock)"
      else
        DOCKER_GID="$(ls -ln /var/run/docker.sock 2>/dev/null | awk '{print $4}')"
      fi
      [ -z "$DOCKER_GID" ] && DOCKER_GID=999
      echo "✅ Docker group ID: $DOCKER_GID"
    else
      echo "❌ /var/run/docker.sock not found; using fallback GID 999"
      DOCKER_GID=999
    fi

    # 1) Wait for compose file
    echo "🔍 Waiting for compose file: $COMPOSE_FILE"
    for i in $(seq 1 180); do
      [ -f "$COMPOSE_FILE" ] && { echo "✅ Found compose"; break; }
      sleep 1
    done
    [ -f "$COMPOSE_FILE" ] || { echo "❌ Compose not found; exiting watcher"; exit 0; }

    # 2) Ensure volumes/envs in service section
    SERVICE_SECTION="$(awk '/^[[:space:]]*yunderagithubcompiler:/,/^[[:space:]]*x-casaos:/ {print}' "$COMPOSE_FILE")"

    echo "$SERVICE_SECTION" | grep -q "/var/run/docker.sock:/var/run/docker.sock" || {
      echo "🔧 Inserting docker.sock volume mount..."
      cp "$COMPOSE_FILE" "$COMPOSE_FILE.backup" 2>/dev/null || true
      awk '
        BEGIN{in_svc=0; inserted=0}
        /^[[:space:]]*yunderagithubcompiler:/ {in_svc=1}
        in_svc && /^[[:space:]]*volumes:[[:space:]]*$/ {
          print; print "            - type: bind"
          print "              source: /var/run/docker.sock"
          print "              target: /var/run/docker.sock"
          inserted=1; next
        }
        in_svc && /^[[:space:]]*networks:[[:space:]]*$/ && !inserted {
          print "        volumes:"
          print "            - type: bind"
          print "              source: /var/run/docker.sock"
          print "              target: /var/run/docker.sock"
          print; inserted=1; next
        }
        {print}
      ' "$COMPOSE_FILE" > "$COMPOSE_FILE.tmp" && mv "$COMPOSE_FILE.tmp" "$COMPOSE_FILE"
    }

    grep -q 'DOCKER_GID:' "$COMPOSE_FILE" || sed -i 's/WEBUI_PORT: *"3000"/WEBUI_PORT: "3000"\n            DOCKER_GID: "'"$DOCKER_GID"'"/' "$COMPOSE_FILE"
    grep -q 'AUTH_HASH:'  "$COMPOSE_FILE" || sed -i 's/WEBUI_PORT: *"3000"/WEBUI_PORT: "3000"\n            AUTH_HASH: "__AUTH_HASH__"/' "$COMPOSE_FILE"

    # Add hash to x-casaos.index if missing
    grep -q '^ *index: */\?hash=' "$COMPOSE_FILE" || sed -i 's|^\( *index: *\)/.*$|\1/?hash=__AUTH_HASH__|' "$COMPOSE_FILE"

    # Remove the single-line pre-install-cmd (CasaOS flattens it)
    sed -i '/^[[:space:]]*pre-install-cmd:/d' "$COMPOSE_FILE"

    # 3) Wait for container stability (avoid racing CasaOS)
    echo "⏳ Waiting for container to be stable..."
    stable=0
    prev="$(docker ps -a --filter "name=^${SERVICE_NAME}$" --format "{{.ID}} {{.Status}}")"
    for i in $(seq 1 30); do
      cur="$(docker ps -a --filter "name=^${SERVICE_NAME}$" --format "{{.ID}} {{.Status}}")"
      if [ -n "$cur" ] && [ "$cur" = "$prev" ] && echo "$cur" | grep -q 'Up '; then
        stable=$((stable+1))
      else
        stable=0
      fi
      [ $stable -ge 3 ] && { echo "✅ Stable"; break; }
      prev="$cur"
      sleep 2
    done

    # 4) Use the SAME compose project as CasaOS to avoid name conflicts
    PROJECT="$(docker inspect -f '{{ index .Config.Labels "com.docker.compose.project" }}' "${SERVICE_NAME}" 2>/dev/null || true)"
    [ -z "$PROJECT" ] && PROJECT="$(basename "$APP_DIR")"
    echo "🔧 Using compose project: $PROJECT"

    cd "$APP_DIR"

    compose_up() {
      if docker compose version >/dev/null 2>&1; then
        docker compose -p "$PROJECT" up -d --force-recreate
      elif command -v docker-compose >/dev/null 2>&1; then
        docker-compose -p "$PROJECT" up -d --force-recreate
      else
        echo "⚠️ No docker compose available; skipping restart"
        return 0
      fi
    }

    # 5) Recreate; if conflict persists, stop+rm then up (same project)
    if ! compose_up; then
      echo "⚠️ compose up failed; trying stop+rm then up"
      docker stop "$SERVICE_NAME" >/dev/null 2>&1 || true
      docker rm   "$SERVICE_NAME" >/dev/null 2>&1 || true
      compose_up || echo "❌ compose up failed again"
    fi

    # 6) Verify
    sleep 4
    if docker ps --filter "name=^${SERVICE_NAME}$" --format "{{.Names}}\t{{.Status}}" | grep -q "^${SERVICE_NAME}"; then
      echo "✅ ${SERVICE_NAME} running:"
      docker ps --filter "name=^${SERVICE_NAME}$" --format "{{.Names}}\t{{.Status}}" || true
      docker exec "${SERVICE_NAME}" test -S /var/run/docker.sock 2>/dev/null \
        && echo "✅ docker.sock accessible in container" \
        || echo "❌ docker.sock not accessible in container"
    else
      echo "❌ ${SERVICE_NAME} is not running after restart"
    fi

    echo "🏁 Watcher script completed at $(date)"
    EOF

    # Inject the runtime AUTH_HASH into the watcher text
    sed -i "s|__AUTH_HASH__|$AUTH_HASH|g" /tmp/yundera-docker-sock-fixer.sh
    chmod +x /tmp/yundera-docker-sock-fixer.sh
    nohup /tmp/yundera-docker-sock-fixer.sh > /tmp/yundera-docker-sock-fixer.log 2>&1 &
    echo "🚀 Docker.sock fixer launched, logs: /tmp/yundera-docker-sock-fixer.log"
